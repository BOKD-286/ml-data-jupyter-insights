
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://habrastorage.org/webt/mg/vd/hd/mgvdhdbnd99fhsk6htcn9jqzt88.png\" width=50%/>\n",
    "\n",
    "## <center>  Tutorial on text classification <br>  <br> Analyzing Amazon product reviews\n",
    "<center> Yury Kashnitskiy, Data Science Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be analyzing Amazon products [reviews](http://jmcauley.ucsd.edu/data/amazon/). We took a sample of 100k grocery reviews. The prepared zipped `.csv` file is [here](https://drive.google.com/file/d/1fUSV3GrFzKkpY7tvbp3-tNHqb-Veo4xY/view?usp=sharing).\n",
    "\n",
    "**Outline:**\n",
    "\n",
    "1. Simple text features<br>\n",
    "   1.1. Bag of Words<br>\n",
    "   1.2. Tf-Idf vectorization<br>\n",
    "2. Simple text classification<br>\n",
    "3. Understanding the model<br>\n",
    "   3.1. Confusion matrix<br>\n",
    "   3.2. Visualizing coefficients<br>\n",
    "   3.3. ELI5 (\"Explain Like I'm 5\")<br>\n",
    "4. Hierarchical text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = '/home/yorko/Documents/data/amazon_reviews_sample100k_grocery.csv.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some necessary imports\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PATH_TO_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>productId</th>\n",
       "      <th>Title</th>\n",
       "      <th>userId</th>\n",
       "      <th>Helpfulness</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Text</th>\n",
       "      <th>Cat1</th>\n",
       "      <th>Cat2</th>\n",
       "      <th>Cat3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B0000DF3IX</td>\n",
       "      <td>Paprika Hungarian Sweet</td>\n",
       "      <td>A244MHL2UN2EYL</td>\n",
       "      <td>0/0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1127088000</td>\n",
       "      <td>While in Hungary we were given a recipe for Hu...</td>\n",
       "      <td>grocery  gourmet food</td>\n",
       "      <td>herbs</td>\n",
       "      <td>spices  seasonings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B0002QF1LK</td>\n",
       "      <td>Quaker Honey Graham Oh's 10.5 oz - (6 pack)</td>\n",
       "      <td>A3FL7SXVYMC5NR</td>\n",
       "      <td>3/3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1138147200</td>\n",
       "      <td>Without a doubt, I would recommend this wholes...</td>\n",
       "      <td>grocery  gourmet food</td>\n",
       "      <td>breakfast foods</td>\n",
       "      <td>cereals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B0002QF1LK</td>\n",
       "      <td>Quaker Honey Graham Oh's 10.5 oz - (6 pack)</td>\n",
       "      <td>A12IDQSS4OW33B</td>\n",
       "      <td>3/3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1118016000</td>\n",
       "      <td>This cereal is so sweet....yet so good for you...</td>\n",
       "      <td>grocery  gourmet food</td>\n",
       "      <td>breakfast foods</td>\n",
       "      <td>cereals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B0002QF1LK</td>\n",
       "      <td>Quaker Honey Graham Oh's 10.5 oz - (6 pack)</td>\n",
       "      <td>A2GZKHC1M4PKF4</td>\n",
       "      <td>2/2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1206489600</td>\n",
       "      <td>Man I love Oh's cereal. It is really great to ...</td>\n",
       "      <td>grocery  gourmet food</td>\n",
       "      <td>breakfast foods</td>\n",
       "      <td>cereals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B0002QF1LK</td>\n",
       "      <td>Quaker Honey Graham Oh's 10.5 oz - (6 pack)</td>\n",
       "      <td>AUGT2DOGKLHIN</td>\n",
       "      <td>2/2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1177545600</td>\n",
       "      <td>And I've tried alot of cereals. This is by far...</td>\n",
       "      <td>grocery  gourmet food</td>\n",
       "      <td>breakfast foods</td>\n",
       "      <td>cereals</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    productId                                        Title          userId  \\\n",
       "0  B0000DF3IX                      Paprika Hungarian Sweet  A244MHL2UN2EYL   \n",
       "1  B0002QF1LK  Quaker Honey Graham Oh's 10.5 oz - (6 pack)  A3FL7SXVYMC5NR   \n",
       "2  B0002QF1LK  Quaker Honey Graham Oh's 10.5 oz - (6 pack)  A12IDQSS4OW33B   \n",
       "3  B0002QF1LK  Quaker Honey Graham Oh's 10.5 oz - (6 pack)  A2GZKHC1M4PKF4   \n",
       "4  B0002QF1LK  Quaker Honey Graham Oh's 10.5 oz - (6 pack)   AUGT2DOGKLHIN   \n",
       "\n",
       "  Helpfulness  Score        Time  \\\n",
       "0         0/0    5.0  1127088000   \n",
       "1         3/3    5.0  1138147200   \n",
       "2         3/3    5.0  1118016000   \n",
       "3         2/2    3.0  1206489600   \n",
       "4         2/2    5.0  1177545600   \n",
       "\n",
       "                                                Text                    Cat1  \\\n",
       "0  While in Hungary we were given a recipe for Hu...   grocery  gourmet food   \n",
       "1  Without a doubt, I would recommend this wholes...   grocery  gourmet food   \n",
       "2  This cereal is so sweet....yet so good for you...   grocery  gourmet food   \n",
       "3  Man I love Oh's cereal. It is really great to ...   grocery  gourmet food   \n",
       "4  And I've tried alot of cereals. This is by far...   grocery  gourmet food   \n",
       "\n",
       "              Cat2                Cat3  \n",
       "0            herbs  spices  seasonings  \n",
       "1  breakfast foods             cereals  \n",
       "2  breakfast foods             cereals  \n",
       "3  breakfast foods             cereals  \n",
       "4  breakfast foods             cereals  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99982, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['productId', 'Title', 'userId', 'Helpfulness', 'Score', 'Time', 'Text',\n",
       "       'Cat1', 'Cat2', 'Cat3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these 10 columns we'll use only 3 now:\n",
    " - Text - review on the product\n",
    " - Cat2 - label of category 2 for this product\n",
    " - Cat3 - label of category 3 for this product\n",
    " \n",
    "There's a taxonomy (hierarchical catalog) of all products with 3 categories (a.k.a. levels). Based on the review, we're going to classify it into one of level 2 categories (i.e. predicting `Cat2`) and level 3 categories (i.e. predicting `Cat3`). \n",
    "\n",
    "We're not intrested anymore in `Cat1` because here we chose only grocery. So we have 16 `Cat2` categories and  157 `Cat3` categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' grocery  gourmet food'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Cat1'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pantry staples                       27291\n",
       "beverages                            23440\n",
       "snack food                           12724\n",
       "candy chocolate                      11433\n",
       "breakfast foods                       6248\n",
       "breads  bakery                        4240\n",
       "cooking  baking supplies              2444\n",
       "herbs                                 2069\n",
       "gourmet gifts                         1939\n",
       "fresh flowers  live indoor plants     1811\n",
       "baby food                             1270\n",
       "meat  poultry                         1268\n",
       "meat  seafood                         1250\n",
       "produce                               1196\n",
       "sauces  dips                           845\n",
       "dairy  eggs                            514\n",
       "Name: Cat2, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Cat2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Cat3'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Simple text features\n",
    "#### 1.1. Bag of Words \n",
    "\n",
    "*The following explanation of Bag of Words and Tf-Idf is based on [this](https://www.kaggle.com/kashnitsky/topic-6-feature-engineering-and-feature-selection) notebook from our course [mlcourse.ai](https://mlcourse.a).*\n",
    "\n",
    "<img src=\"https://habrastorage.org/webt/r7/sq/my/r7sqmyj1nmqmzltaftt40zi7-gw.png\" width=60%/>\n",
    "\n",
    "The easiest way to convert text to features is called Bag of Words: we create a vector with the length of the vocabulary, compute the number of occurrences of each word in the text, and place that number of occurrences in the appropriate position in the vector. The process described looks simpler in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: [(0, 'i'), (1, 'dog'), (2, 'and'), (3, 'a'), (4, 'cat'), (5, 'you'), (6, 'have')]\n",
      "Vectors:\n",
      "[1. 0. 0. 1. 1. 0. 1.]\n",
      "[0. 1. 0. 1. 0. 1. 1.]\n",
      "[1. 1. 2. 2. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "texts = ['i have a cat', \n",
    "         'you have a dog', \n",
    "         'you and i have a cat and a dog']\n",
    "\n",
    "vocabulary = list(enumerate(set([word for sentence in texts \n",
    "                                 for word in sentence.split()])))\n",
    "print('Vocabulary:', vocabulary)\n",
    "\n",
    "def vectorize(text): \n",
    "    vector = np.zeros(len(vocabulary)) \n",
    "    for i, word in vocabulary:\n",
    "        num = 0 \n",
    "        for w in text: \n",
    "            if w == word: \n",
    "                num += 1 \n",
    "        if num: \n",
    "            vector[i] = num \n",
    "    return vector\n",
    "\n",
    "print('Vectors:')\n",
    "for sentence in texts: \n",
    "    print(vectorize(sentence.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix:\n",
      " [[0 1 0 1 0]\n",
      " [0 0 1 1 1]\n",
      " [2 1 1 1 1]]\n",
      "Vocabulary\n",
      "{'and': 0, 'cat': 1, 'dog': 2, 'have': 3, 'you': 4}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer()\n",
    "print('Feature matrix:\\n {}'.format(vect.fit_transform(texts).toarray()))\n",
    "print('Vocabulary')\n",
    "pprint(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using algorithms like Bag of Words, we lose the order of the words in the text, which means that the texts \"i have no cows\" and \"no, i have cows\" will appear identical after vectorization when, in fact, they have the opposite meaning. To avoid this problem, we can revisit our tokenization step and use N-grams (the *sequence* of N consecutive tokens) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix:\n",
      " [[0 0 0 1 0 0 1 1 0 0 0 0]\n",
      " [0 0 0 0 0 1 1 0 1 1 0 1]\n",
      " [2 1 1 1 1 1 1 1 0 1 1 0]]\n",
      "Vocabulary\n",
      "{'and': 0,\n",
      " 'and dog': 1,\n",
      " 'and have': 2,\n",
      " 'cat': 3,\n",
      " 'cat and': 4,\n",
      " 'dog': 5,\n",
      " 'have': 6,\n",
      " 'have cat': 7,\n",
      " 'have dog': 8,\n",
      " 'you': 9,\n",
      " 'you and': 10,\n",
      " 'you have': 11}\n"
     ]
    }
   ],
   "source": [
    "# the same but with bigrams\n",
    "vect2 = CountVectorizer(ngram_range=(1, 2))\n",
    "print('Feature matrix:\\n {}'.format(vect2.fit_transform(texts).toarray()))\n",
    "print('Vocabulary')\n",
    "pprint(vect2.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Tf-Idf \n",
    "Adding onto the Bag of Words idea: words that are rarely found in the corpus (in all the documents of this dataset) but are present in this particular document might be more important. Then it makes sense to increase the weight of more domain-specific words to separate them out from common words. This approach is called TF-IDF (term frequency-inverse document frequency), which cannot be written in a few lines, so you should look into the details in references such as [this wiki](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). The default option is as follows:\n",
    "\n",
    "$$ \\large idf(t,D) = \\log\\frac{\\mid D\\mid}{df(d,t)+1} $$\n",
    "\n",
    "$$ \\large tfidf(t,d,D) = tf(t,d) \\times idf(t,D) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Simple text classification\n",
    "\n",
    "For now, we'll only take a look at 16 level 2 categories. We'll be doing a 16-class classification with logistic regression and Tf-Idf vectorization. Here we resort to Sklearn pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build bigrams, put a limit on maximal number of features\n",
    "# and minimal word frequency\n",
    "tf_idf = TfidfVectorizer(ngram_range=(1, 2), max_features=50000, min_df=2)\n",
    "# multinomial logistic regression a.k.a softmax classifier\n",
    "logit = LogisticRegression(C=1e2, n_jobs=4, solver='lbfgs', \n",
    "                           random_state=17, multi_class='multinomial',\n",
    "                           verbose=1)\n",
    "# sklearn's pipeline\n",
    "tfidf_logit_pipeline = Pipeline([('tf_idf', tf_idf), \n",
    "                                 ('logit', logit)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we only use review text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, y = df['Text'], df['Cat2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split data into training and validation parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, valid_texts, y_train, y_valid = \\\n",
    "        train_test_split(texts, y, random_state=17,\n",
    "                         stratify=y, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.1 s, sys: 300 ms, total: 10.4 s\n",
      "Wall time: 46.3 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   1 out of   1 | elapsed:   36.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tf_idf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=50000, min_df=2,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=Tru... penalty='l2', random_state=17, solver='lbfgs',\n",
       "          tol=0.0001, verbose=1, warm_start=False))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tfidf_logit_pipeline.fit(train_texts, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.42 s, sys: 7.79 ms, total: 2.43 s\n",
      "Wall time: 2.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "valid_pred = tfidf_logit_pipeline.predict(valid_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7564810369659145"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_valid, valid_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Understanding the model\n",
    "#### 3.1. Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(actual, predicted, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix', figsize=(7,7),\n",
    "                          cmap=plt.cm.Blues, path_to_save_fig=None):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",